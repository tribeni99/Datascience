{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1.python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418af2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First install the selenium library\n",
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09cba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:/Users/tribe/Downloads/chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80643c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb361d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding elements for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "search_loc = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19784b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2594cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will see in the window opened or not.lets check it by specifying the url of the webpage to be scraped\n",
    "url = \"https://www.naukri.com/data-analyst-data-analyst-jobs?k=data%20analyst%2C%20data%20analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54769ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets first creat 4 empty lists.\n",
    "In these lists the data will be stored while scraping. We have created 4 empty lists for 4 features which we have to exrat\n",
    "1.job_titles2.locations_list3.company_names4.experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "locations_list=[]\n",
    "company_names=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3d65ba",
   "metadata": {},
   "source": [
    "First we will extract all the tags where we have the job titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so lets extract all the tags having the job_titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2390784",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles.\n",
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58047f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets text of the job titles is inside the tags extracted above.\n",
    "#so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "job_titles=[]\n",
    "for i in titles_tags:\n",
    "    job_title=i.text\n",
    "    job_titles.append(job_title)\n",
    "job_titles[0:10]   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601a933",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_list=[]\n",
    "for i in locations_tags:\n",
    "    location=i.text\n",
    "    locations_list.append(location)\n",
    "locations_list[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949fd94",
   "metadata": {},
   "source": [
    "Now we will extract the html tags where we have the company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e534b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32735f20",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names=[]\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b000bb",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the experience required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experience_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c9c60d",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the experience required data.Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55986b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_list=[]\n",
    "for i in experience_tags:\n",
    "    experience=i.text\n",
    "    experience_list.append(experience)\n",
    "experience_list[0:10]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2805c4d9",
   "metadata": {},
   "source": [
    "So,now we have extracted the data required from the webpage and stored them in the 4 lists mentioned above.Now before creating a dataframe from these lists.Lets first check the length of each of the list.Beacause if the length of all of the lists are not equal, then a dataframe cannot be formed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad599b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_titles),len(locations_list),len(company_names),len(experience_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e71cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles\n",
    "jobs['location']=locations_list\n",
    "jobs['company']=company_names\n",
    "jobs['experience_required']=experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a373d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f955346",
   "metadata": {},
   "source": [
    "Q2.python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0704ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:/Users/tribe/Downloads/chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab951a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ddc5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding elements for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "search_loc = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93952be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will see in the window opened or not.lets check it by specifying the url of the webpage to be scraped\n",
    "url = \"https://www.naukri.com/data-scientist-jobs-in-bangalore?k=data%20scientist&l=bangalore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34db281",
   "metadata": {},
   "source": [
    "#Now lets first creat 4 empty lists.\n",
    "In these lists the data will be stored while scraping. We have created 4 empty lists for 4 features which we have to exrat\n",
    "1.job_titles2.locations_list3.company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "locations_list=[]\n",
    "company_names=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so lets extract all the tags having the job_titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed9ad0",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles. Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70173684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets text of the job titles is inside the tags extracted above.\n",
    "#so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "job_titles=[]\n",
    "for i in titles_tags:\n",
    "    job_title=i.text\n",
    "    job_titles.append(job_title)\n",
    "job_titles[0:10]   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af6699",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e13f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_list=[]\n",
    "for i in locations_tags:\n",
    "    location=i.text\n",
    "    locations_list.append(location)\n",
    "locations_list[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f3d8a",
   "metadata": {},
   "source": [
    "Now we will extract the html tags where we have the company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a264db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e3202",
   "metadata": {},
   "source": [
    "Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names=[]\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f23de",
   "metadata": {},
   "source": [
    "So,now we have extracted the data required from the webpage and stored them in the 3 lists mentioned above.Now before creating a dataframe from these lists.Lets first check the length of each of the list.Beacause if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_titles),len(locations_list),len(company_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c58088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles\n",
    "jobs['location']=locations_list\n",
    "jobs['company']=company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbed498",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c2e55",
   "metadata": {},
   "source": [
    "Q3.You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:/Users/tribe/Downloads/chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac19c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1642855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding elements for job search bar\n",
    "search_job = driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3d7d5",
   "metadata": {},
   "source": [
    "#Now lets first creat 4 empty lists.\n",
    "In these lists the data will be stored while scraping.\n",
    "We have created 4 empty lists for 4 features which we have to exrat\n",
    "1.job_titles2.locations_list3.company_names4.experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1282c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]\n",
    "locations_list=[]\n",
    "company_names=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317925b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so lets extract all the tags having the job_titles\n",
    "titles_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "titles_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32454fe7",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there are the job titles. Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109552aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets text of the job titles is inside the tags extracted above.\n",
    "#so we will run a loop to iterate over the tags extracted above and extract the tags\n",
    "job_titles=[]\n",
    "for i in titles_tags:\n",
    "    job_title=i.text\n",
    "    job_titles.append(job_title)\n",
    "job_titles[0:10]   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69237fb7",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the location of the job data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f357cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "locations_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a63a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_list=[]\n",
    "for i in locations_tags:\n",
    "    location=i.text\n",
    "    locations_list.append(location)\n",
    "locations_list[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7755f",
   "metadata": {},
   "source": [
    "Now we will extract the html tags where we have the company names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f027d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "companies_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50134c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names=[]\n",
    "for i in companies_tags:\n",
    "    company_name=i.text\n",
    "    company_names.append(company_name)\n",
    "company_names[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274a7ff",
   "metadata": {},
   "source": [
    "Now we will extract all the html tags where we have the experience required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b66f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "experience_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf1ce7",
   "metadata": {},
   "source": [
    "Now we have all the tags in which there is the experience required data.Now we will extract the text from these tags one by one by looping over these tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_list=[]\n",
    "for i in experience_tags:\n",
    "    experience=i.text\n",
    "    experience_list.append(experience)\n",
    "experience_list[0:10]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7476189",
   "metadata": {},
   "source": [
    "So,now we have extracted the data required from the webpage and stored them in the 3 lists mentioned above.Now before creating a dataframe from these lists.Lets first check the length of each of the list.Beacause if the length of all of the lists are not equal, then a dataframe cannot be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23483b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_titles),len(locations_list),len(company_names),len(experience_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles\n",
    "jobs['location']=locations_list\n",
    "jobs['company']=company_names\n",
    "jobs['experience']=experience_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecdbbe2",
   "metadata": {},
   "source": [
    "Q4.Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom of the page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85068d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets connect to the web driver\n",
    "driver1 = webdriver.Chrome(r\"C:/Users/tribe/Downloads/chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eee730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef5a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver1.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d259b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1=driver1.find_element_by_xpath(\"//input[@title='Search for products, brands and more']\")\n",
    "ch1.send_keys('sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "button=driver1.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f946d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "srh_nam=driver1.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "srh_des=driver1.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "srh_price=driver1.find_elements_by_xpath(\"//div[@class='_30jeq3']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d066785",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(srh_nam),len(srh_des),len(srh_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13158ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_nam=[]\n",
    "Product_Desc=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a19dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_nam.clear()\n",
    "Product_Desc.clear()\n",
    "Price.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rh in srh_nam:\n",
    "    rh1=rh.text\n",
    "    Brand_nam.append(rh1)\n",
    "for de in srh_des:\n",
    "    de1=de.text\n",
    "    Product_Desc.append(de1)\n",
    "for pr in srh_price:\n",
    "    pr1=pr.text\n",
    "    Price.append(pr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "button1=driver1.find_element_by_xpath(\"//a[@class='_1LKTO3']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603856cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "button1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_nam1=[]\n",
    "Product_desc1=[]\n",
    "Price1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_nam1.clear()\n",
    "Product_desc1.clear()\n",
    "Price1.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd39cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "srh_nam1=driver1.find_elements_by_xpath(\"//div[@class='_2WKVRV']\")\n",
    "srh_des1=driver1.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "srh_price1=driver1.find_elements_by_xpath(\"//div[@class='_30jeq3']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len('srh_nam1'),len('srh_des1'),len('srh_price1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a52292",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rh1 in srh_nam1:\n",
    "    rh2=rh1.text\n",
    "    Brand_nam1.append(rh2)\n",
    "for de1 in srh_des1:\n",
    "    de2=de1.text\n",
    "    Product_Desc1.append(de2)\n",
    "for pr1 in srh_price1:\n",
    "    pr2=pr1.text\n",
    "    Price1.append(pr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9551b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "button1.click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_nam2=[]\n",
    "Product_Desc2=[]\n",
    "Price2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a738d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "srh_nam2=driver1.find_elements_by_xpath(\"//div[@class='_2WKVRV']\")\n",
    "srh_des2=driver1.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "srh_price2=driver1.find_elements_by_xpath(\"//div[@class='_30jeq3']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len('srh_nam2'),len('srh_des2'),len('srh_price2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b42796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rh2 in srh_nam2:\n",
    "    rh3=rh2.text\n",
    "    Brand_nam2.append(rh3)\n",
    "    \n",
    "for de2 in srh_des2:\n",
    "    de3=de2.text\n",
    "    Product_Desc2.append(de3)\n",
    "    \n",
    "for pr2 in srh_price2:\n",
    "    pr3=pr2.text\n",
    "    Price2.append(pr3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ccb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbrd=[]\n",
    "fpdes=[]\n",
    "fpric=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e375bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbrd=Brand_nam+Brand_nam1+Brand_nam2\n",
    "fpdes=Product_Desc+Product_Desc1+Product_Desc2\n",
    "fpric=Price+Price1+Price2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame((\"Brand\":fbrd,\"Product_Des\":fpdes,\"Price\":fpric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cef730",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
